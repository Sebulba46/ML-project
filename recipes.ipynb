{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исследование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingRegressor, BaggingRegressor, VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сделаем свой разделитель на выборки: тренировочную, тестовую и валидационную"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValidationTest:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=21, stratify=y_train)\n",
    "        return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Автоматический вычислитель лучшей модели с помощью GridSearchCV и cross validation, к сожалению он не позволяет выбрать оптимальную модель, т.е. сильно долго работающие модели будут тоже учитываться в рейтинге"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection:\n",
    "    def __init__(self, grids, grid_dict):\n",
    "        self.result = None\n",
    "        self.grids = grids\n",
    "        self.grid_dict = grid_dict\n",
    "\n",
    "    def choose(self, X_train, y_train, X_valid, y_valid):\n",
    "        out = pd.DataFrame({'model_num': self.grid_dict.keys(),\n",
    "                                'model': self.grid_dict.values()})\n",
    "\n",
    "        def select_params(row):\n",
    "            ind = row['model_num']\n",
    "            print(f'Estimator: {self.grid_dict[ind]}')\n",
    "\n",
    "            self.grids[ind].fit(X_train, y_train)\n",
    "            print(f'Best params: {self.grids[ind].best_params_}')\n",
    "            y_predict = self.grids[ind].predict(X_valid)\n",
    "\n",
    "            score = r2_score(y_valid, y_predict)\n",
    "            print(f'Best training r2 score: {self.grids[ind].best_score_}')\n",
    "            print(f'Validation set r2 score for best params: {score}')\n",
    "            print('-'*55)\n",
    "\n",
    "            return self.grids[ind].best_params_, score\n",
    "\n",
    "\n",
    "        out[['params', 'valid_score']] = out.progress_apply(select_params,\n",
    "                                                                axis=1,\n",
    "                                                                result_type='expand')\n",
    "        self.result = out\n",
    "        best_clf = out.model[out.valid_score.idxmax()]\n",
    "\n",
    "        print(f'Regressor with best validation set r2 score: {best_clf}')\n",
    "\n",
    "    def best_results(self):\n",
    "        return self.result.drop(columns='model_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Убираем все поля, которые не относится к ингредиентам"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                             title  rating  calories  protein  \\\n0                  Lentil, Apple, and Turkey Wrap    2.500     426.0     30.0   \n1      Boudin Blanc Terrine with Red Onion Confit    4.375     403.0     18.0   \n2                    Potato and Fennel Soup Hodge    3.750     165.0      6.0   \n4                        Spinach Noodle Casserole    3.125     547.0     20.0   \n5                                   The Best Blts    4.375     948.0     19.0   \n...                                            ...     ...       ...      ...   \n20047                              Parmesan Puffs    3.125      28.0      2.0   \n20048              Artichoke and Parmesan Risotto    4.375     671.0     22.0   \n20049                       Turkey Cream Puff Pie    4.375     563.0     31.0   \n20050     Snapper on Angel Hair with Citrus Cream    4.375     631.0     45.0   \n20051  Baked Ham with Marmalade-Horseradish Glaze    4.375     560.0     73.0   \n\n        fat  sodium  almond  amaretto  anchovy  anise  ...  whiskey  \\\n0       7.0   559.0     0.0       0.0      0.0    0.0  ...      0.0   \n1      23.0  1439.0     0.0       0.0      0.0    0.0  ...      0.0   \n2       7.0   165.0     0.0       0.0      0.0    0.0  ...      0.0   \n4      32.0   452.0     0.0       0.0      0.0    0.0  ...      0.0   \n5      79.0  1042.0     0.0       0.0      0.0    0.0  ...      0.0   \n...     ...     ...     ...       ...      ...    ...  ...      ...   \n20047   2.0    64.0     0.0       0.0      0.0    0.0  ...      0.0   \n20048  28.0   583.0     0.0       0.0      0.0    0.0  ...      0.0   \n20049  38.0   652.0     0.0       0.0      0.0    0.0  ...      0.0   \n20050  24.0   517.0     0.0       0.0      0.0    0.0  ...      0.0   \n20051  10.0  3698.0     0.0       0.0      0.0    0.0  ...      0.0   \n\n       white wine  whole wheat  wild rice  wine  yellow squash  yogurt  yuca  \\\n0             0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n1             0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n2             0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n4             0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n5             0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n...           ...          ...        ...   ...            ...     ...   ...   \n20047         0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n20048         0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n20049         0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n20050         0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n20051         0.0          0.0        0.0   0.0            0.0     0.0   0.0   \n\n       zucchini  turkey  \n0           0.0     1.0  \n1           0.0     0.0  \n2           0.0     0.0  \n4           0.0     0.0  \n5           0.0     0.0  \n...         ...     ...  \n20047       0.0     0.0  \n20048       0.0     0.0  \n20049       0.0     1.0  \n20050       0.0     0.0  \n20051       0.0     0.0  \n\n[15864 rows x 456 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>rating</th>\n      <th>calories</th>\n      <th>protein</th>\n      <th>fat</th>\n      <th>sodium</th>\n      <th>almond</th>\n      <th>amaretto</th>\n      <th>anchovy</th>\n      <th>anise</th>\n      <th>...</th>\n      <th>whiskey</th>\n      <th>white wine</th>\n      <th>whole wheat</th>\n      <th>wild rice</th>\n      <th>wine</th>\n      <th>yellow squash</th>\n      <th>yogurt</th>\n      <th>yuca</th>\n      <th>zucchini</th>\n      <th>turkey</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lentil, Apple, and Turkey Wrap</td>\n      <td>2.500</td>\n      <td>426.0</td>\n      <td>30.0</td>\n      <td>7.0</td>\n      <td>559.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Boudin Blanc Terrine with Red Onion Confit</td>\n      <td>4.375</td>\n      <td>403.0</td>\n      <td>18.0</td>\n      <td>23.0</td>\n      <td>1439.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Potato and Fennel Soup Hodge</td>\n      <td>3.750</td>\n      <td>165.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>165.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Spinach Noodle Casserole</td>\n      <td>3.125</td>\n      <td>547.0</td>\n      <td>20.0</td>\n      <td>32.0</td>\n      <td>452.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The Best Blts</td>\n      <td>4.375</td>\n      <td>948.0</td>\n      <td>19.0</td>\n      <td>79.0</td>\n      <td>1042.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20047</th>\n      <td>Parmesan Puffs</td>\n      <td>3.125</td>\n      <td>28.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>64.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20048</th>\n      <td>Artichoke and Parmesan Risotto</td>\n      <td>4.375</td>\n      <td>671.0</td>\n      <td>22.0</td>\n      <td>28.0</td>\n      <td>583.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20049</th>\n      <td>Turkey Cream Puff Pie</td>\n      <td>4.375</td>\n      <td>563.0</td>\n      <td>31.0</td>\n      <td>38.0</td>\n      <td>652.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>20050</th>\n      <td>Snapper on Angel Hair with Citrus Cream</td>\n      <td>4.375</td>\n      <td>631.0</td>\n      <td>45.0</td>\n      <td>24.0</td>\n      <td>517.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20051</th>\n      <td>Baked Ham with Marmalade-Horseradish Glaze</td>\n      <td>4.375</td>\n      <td>560.0</td>\n      <td>73.0</td>\n      <td>10.0</td>\n      <td>3698.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>15864 rows × 456 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('epi_r.csv')\n",
    "data = data.drop(columns=['#cakeweek', '#wasteless', '22-minute meals', '3-ingredient recipes', '30 days of groceries', 'advance prep required', 'alaska', 'alabama', 'alcoholic', 'non-alcoholic', 'no meat, no problem', 'anniversary', 'anthony bourdain', 'appetizer', 'arizona', 'aspen', 'atlanta', 'australia', 'back to school', 'backyard bbq', 'bake', 'bastille day', 'birthday', 'blender', 'boil', 'bon appétit', 'bon app��tit', 'boston', 'breakfast', 'brooklyn', 'buffet', 'bulgaria', 'burrito', 'cake', 'california', 'cambridge', 'camping', 'canada', 'caraway', 'champagne', 'chicago', 'chile', 'chill', 'christmas', 'christmas eve', 'cinco de mayo', 'cocktail', 'cocktail party', 'house cocktail', 'colorado', 'columbus', 'connecticut', 'cook like a diner', 'cookbook critic', 'cookie', 'cookies', 'cornmeal', 'costa mesa', 'cr��me de cacao', 'cuba', 'cupcake', 'dairy', 'dairy free', 'dallas', 'date', 'deep-fry', 'denver', 'dessert', 'digestif', 'dip', 'diwali', 'dominican republic', 'double boiler', 'dorie greenspan', 'drink', 'drinks', 'easter', 'eau de vie', 'edible gift', 'egg nog', 'eggplant', 'egypt', 'emeril lagasse', 'england', 'engagement party', 'entertaining', 'epi + ushg', 'epi loves the microwave', 'fall', 'family reunion', 'fat free', 'father\\'s day', 'flaming hot summer', 'florida', 'food processor', 'fourth of july', 'france', 'frankenrecipe', 'freeze/chill', 'freezer food', 'friendsgiving', 'frittata', 'fritter', 'frozen dessert', 'fry', 'fruit', 'game', 'georgia', 'germany', 'graduation', 'grill', 'grill/barbecue', 'oregon', 'organic', 'oscars', 'oyster', 'pacific palisades', 'paleo', 'pan-fry', 'pancake', 'party', 'pasadena', 'pasta', 'pasta maker', 'pastry', 'peanut free', 'pennsylvania', 'pernod', 'persian new year', 'peru', 'pescatarian', 'philippines', 'phyllo/puff pastry dough', 'picnic', 'pie', 'pittsburgh', 'poach', 'poker/game night', 'port', 'portland', 'pot pie', 'potato salad', 'potluck', 'poultry', 'poultry sausage', 'pressure cooker', 'providence', 'quick & easy', 'quinoa', 'ramadan', 'ramekin', 'raw', 'rhode island', 'roast', 'sage', 'salad', 'salad dressing', 'san francisco', 'sandwich', 'sandwich theory', 'seafood', 'seattle', 'self', 'shavuot', 'shower', 'side', 'simmer', 'skewer', 'slow cooker', 'smoker', 'smoothie', 'snapper', 'soup/stew', 'south carolina', 'soy free', 'spain', 'spring', 'spritzer', 'st. patrick\\'s day', 'st. louis', 'steam', 'stew', 'stir-fry', 'stock', 'stuffing/dressing', 'summer', 'super bowl', 'suzanne goin', 'switzerland', 'taco', 'tailgating', 'tennessee', 'tested & improved', 'texas', 'thanksgiving', 'tree nut free', 'utah', 'valentine\\'s day', 'vegetarian', 'vermont', 'vermouth', 'virginia', 'waffle', 'washington', 'washington, d.c.', 'wedding', 'weelicious', 'west virginia', 'westwood', 'wheat/gluten-free', 'windsor', 'winter', 'wisconsin', 'wok', 'yonkers', 'cookbooks', 'leftovers', 'snack', 'snack week', 'beverly hills', 'quick and healthy', 'mother\\'s day', 'nebraska', 'low cholesterol'])\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Немного о датасете: здесь мы видим:\n",
    "- названия рецептов\n",
    "- рейтинг (вероятнто посчитан на какой-то платформе-соцсети, где пользователи обмениваются рецепты)\n",
    "- количество Ккал в блюде\n",
    "- количество протеина в блюде, по моему, миллиграммы\n",
    "- жирность блюда\n",
    "- количество содиума\n",
    "- И много-много столбцов, названия которых это название ингредиента, т.е. в если в блюде есть этот ингредиент, в его столбце стоит единица\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Делаем целевую переменную - рейтинг блюда и общую выборку"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.rating.round(2)\n",
    "X = data.drop(columns=['rating', 'title', 'calories', 'protein', 'fat', 'sodium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0        2.50\n1        4.38\n2        3.75\n4        3.12\n5        4.38\n         ... \n20047    3.12\n20048    4.38\n20049    4.38\n20050    4.38\n20051    4.38\nName: rating, Length: 15864, dtype: float64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = TrainValidationTest().transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем сделать регрессию с регуляризацией через параметр alpha, он же лямбда из формулы, и рассчитать значение с точностью до 2 знаков до запятой"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_params = {'alpha': range(30, 75)}\n",
    "\n",
    "grid_ridge = GridSearchCV(estimator=Ridge(random_state=21), param_grid=ridge_params, cv=5, n_jobs=8)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "lasso_params = {'selection': ['cyclic', 'random'], 'alpha': range(15, 40), 'tol': [0.2, 0.002]}\n",
    "\n",
    "grid_lasso = GridSearchCV(estimator=Lasso(random_state=21), param_grid=lasso_params, cv=5, n_jobs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Заводим гриды и их обозначение в переменные"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [grid_ridge, grid_lasso]\n",
    "\n",
    "grid_dict = {0: 'Ridge', 1: 'Lasso'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mod = ModelSelection(grids, grid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Высчитываем лучшую модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a6574e1fa3b4438ac9fb03f517006e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Ridge\n",
      "Best params: {'alpha': 30}\n",
      "Best training r2 score: 0.10379502100778604\n",
      "Validation set r2 score for best params: 0.10883405562473392\n",
      "-------------------------------------------------------\n",
      "Estimator: Lasso\n",
      "Best params: {'alpha': 15, 'selection': 'cyclic', 'tol': 0.2}\n",
      "Best training r2 score: -0.0006342399590497205\n",
      "Validation set r2 score for best params: -1.4536590131619676e-07\n",
      "-------------------------------------------------------\n",
      "Regressor with best validation set r2 score: Ridge\n"
     ]
    }
   ],
   "source": [
    "select_mod.choose(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверим RMSE на тестовой выборке"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE на тестовой выборке: 1.2196295186901487\n",
      "R2 score на тестовой выборке: 0.1011671868227576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ridge = Ridge(alpha=30, random_state=21).fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "print(f'RMSE на тестовой выборке: {mean_squared_error(y_test, y_pred, squared=False)}')\n",
    "print(f'R2 score на тестовой выборке: {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем ансамблевые методы, зададим им параметры"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_params = {'weights': [[1, 2], [1, 5], [3, 1], [5, 2]]}\n",
    "\n",
    "grid_vote = GridSearchCV(estimator=VotingRegressor(estimators=[('rg', Ridge(alpha=30, random_state=21)), ('ls', Lasso(alpha=0, selection='random', tol=0.2))]), param_grid=vote_params, cv=5, n_jobs=12)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "bag_params = {'n_estimators': range(10, 30)}\n",
    "\n",
    "grid_bag = GridSearchCV(estimator=BaggingRegressor(base_estimator=Ridge(alpha=30, random_state=21)), param_grid=bag_params, cv=5, n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [grid_bag, grid_vote]\n",
    "\n",
    "grid_dict = {0: 'Bagging', 1: 'Vote'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mod = ModelSelection(grids, grid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd9c20ba909c44ff8ce923fd0ea936dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Bagging\n",
      "Best params: {'n_estimators': 20}\n",
      "Best training r2 score: 0.10456328176647552\n",
      "Validation set r2 score for best params: 0.10564005819957778\n",
      "-------------------------------------------------------\n",
      "Estimator: Vote\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egors\\PycharmProjects\\Egor's property\\venv\\lib\\site-packages\\sklearn\\ensemble\\_base.py:47: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X, y)\n",
      "C:\\Users\\egors\\PycharmProjects\\Egor's property\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'weights': [3, 1]}\n",
      "Best training r2 score: 0.10185532072000483\n",
      "Validation set r2 score for best params: 0.10741739200631295\n",
      "-------------------------------------------------------\n",
      "Regressor with best validation set r2 score: Vote\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egors\\PycharmProjects\\Egor's property\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.960e+03, tolerance: 3.362e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "select_mod.choose(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Получили результаты, смотрим RMSE на тестовой выборке"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egors\\PycharmProjects\\Egor's property\\venv\\lib\\site-packages\\sklearn\\ensemble\\_base.py:47: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X, y)\n",
      "C:\\Users\\egors\\PycharmProjects\\Egor's property\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE на тестовой выборке: 1.2208799943847632\n",
      "R2 score на тестовой выборке: 0.09932311078680178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egors\\PycharmProjects\\Egor's property\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.960e+03, tolerance: 3.362e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "bag = VotingRegressor(estimators=[('rg', Ridge(alpha=30, random_state=21)), ('ls', Lasso(alpha=0, selection='random', tol=0.2))], weights=[3, 1]).fit(X_train, y_train)\n",
    "y_pred = bag.predict(X_test)\n",
    "print(f'RMSE на тестовой выборке: {mean_squared_error(y_test, y_pred, squared=False)}')\n",
    "print(f'R2 score на тестовой выборке: {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Получилось оч плохо, процентный показатель R2 показывает плохие результаты, регрессия пока выглядит как плохой вариант"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем классификацию, для этого округлим значения оценки до классов: (0, 1, 2, 3, 4, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.round()\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = TrainValidationTest().transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поменяем метрику r2_score на accuracy score для классификации"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection:\n",
    "    def __init__(self, grids, grid_dict):\n",
    "        self.result = None\n",
    "        self.grids = grids\n",
    "        self.grid_dict = grid_dict\n",
    "\n",
    "    def choose(self, X_train, y_train, X_valid, y_valid):\n",
    "        out = pd.DataFrame({'model_num': self.grid_dict.keys(),\n",
    "                                'model': self.grid_dict.values()})\n",
    "\n",
    "        def select_params(row):\n",
    "            ind = row['model_num']\n",
    "            print(f'Estimator: {self.grid_dict[ind]}')\n",
    "\n",
    "            self.grids[ind].fit(X_train, y_train)\n",
    "            print(f'Best params: {self.grids[ind].best_params_}')\n",
    "            y_predict = self.grids[ind].predict(X_valid)\n",
    "\n",
    "            score = accuracy_score(y_valid, y_predict)\n",
    "\n",
    "            print(f'Best training accuracy: {self.grids[ind].best_score_}')\n",
    "            print(f'Validation set accuracy score for best params: {score}')\n",
    "            print('-'*55)\n",
    "\n",
    "            return self.grids[ind].best_params_, score\n",
    "\n",
    "\n",
    "        out[['params', 'valid_score']] = out.progress_apply(select_params,\n",
    "                                                                axis=1,\n",
    "                                                                result_type='expand')\n",
    "        self.result = out\n",
    "        best_clf = out.model[out.valid_score.idxmax()]\n",
    "\n",
    "        print(f'Classifier with best validation set accuracy: {best_clf}')\n",
    "\n",
    "    def best_results(self):\n",
    "        return self.result.drop(columns='model_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем три алгоритма: деревья решений, случайный лес и градиентный бустинг"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_params = {'max_depth': range(1, 50), 'class_weight': ('balanced', None), 'criterion': ('entropy', 'gini')}\n",
    "\n",
    "grid_tree = GridSearchCV(estimator=DecisionTreeClassifier(random_state=21), param_grid = dec_tree_params, cv=5, n_jobs=12)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "forest_params = {'n_estimators': [ 160, 170, 165, 195], 'max_depth': range(30, 50), 'class_weight': ('balanced', None), 'criterion': ('entropy', 'gini')}\n",
    "\n",
    "gs_ran_for = GridSearchCV(estimator=RandomForestClassifier(random_state=21), param_grid = forest_params, cv=5, n_jobs=12)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "boosting_params = {'loss': ('log_loss', 'deviance'), 'n_estimators': [50, 100], 'criterion': ('friedman_mse', 'squared_error'), 'max_depth': [20, 22]}\n",
    "\n",
    "gs_gr_boosting = GridSearchCV(estimator=GradientBoostingClassifier(random_state=21), param_grid=boosting_params, cv=5, n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [gs_ran_for, grid_tree, gs_gr_boosting]\n",
    "\n",
    "grid_dict = {0: 'Random Forest', 1: 'Grid tree', 2: 'Gradient boosting'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mod = ModelSelection(grids, grid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405a23b513524dfabb9a01e1165f25af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Random Forest\n",
      "Best params: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 49, 'n_estimators': 170}\n",
      "Best training accuracy: 0.7081362041072732\n",
      "Validation set accuracy score for best params: 0.7073651043717999\n",
      "-------------------------------------------------------\n",
      "Estimator: Grid tree\n",
      "Best params: {'class_weight': None, 'criterion': 'gini', 'max_depth': 12}\n",
      "Best training accuracy: 0.6827228694156825\n",
      "Validation set accuracy score for best params: 0.6809767625049232\n",
      "-------------------------------------------------------\n",
      "Estimator: Gradient boosting\n",
      "Best params: {'criterion': 'friedman_mse', 'loss': 'log_loss', 'max_depth': 20, 'n_estimators': 100}\n",
      "Best training accuracy: 0.6872535308627603\n",
      "Validation set accuracy score for best params: 0.6857030326900354\n",
      "-------------------------------------------------------\n",
      "Classifier with best validation set accuracy: Random Forest\n"
     ]
    }
   ],
   "source": [
    "select_mod.choose(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверим точность на тестовой выборке"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993381657737158"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_for = RandomForestClassifier(class_weight='balanced', random_state=21, criterion='gini', max_depth=40, n_estimators=195, n_jobs=12).fit(X_train, y_train)\n",
    "y_pred = ran_for.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на тестовой выборке точность чутка ниже, чем на валидационной, это хорошо"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Итак, точность довольно неплохая, но нет, смысла делать классификацию по такому количеству классов, так как не особо интуитивно понятно, будет ли блюдо прям совсем невкусное, если у него рейтинг 2, есть же ещё 0 и 1. Так что стоит уменьшить количество классов, сделаем это ниже"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['so-so',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'bad',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'bad',\n 'bad',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'so-so',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'bad',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'bad',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'so-so',\n 'so-so',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'great',\n 'bad',\n 'bad',\n ...]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ['bad' if s in (0, 1) else 'so-so' if s in (2, 3) else 'great' for s in y]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разделим заново основные датасеты"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = TrainValidationTest().transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выберем лучшую модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c0297566ef4951a2b146f548e4033f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Random Forest\n",
      "Best params: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 49, 'n_estimators': 195}\n",
      "Best training accuracy: 0.8238767575486365\n",
      "Validation set accuracy score for best params: 0.8251280031508468\n",
      "-------------------------------------------------------\n",
      "Estimator: Grid tree\n",
      "Best params: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 3}\n",
      "Best training accuracy: 0.8124507085980117\n",
      "Validation set accuracy score for best params: 0.8141000393855848\n",
      "-------------------------------------------------------\n",
      "Estimator: Gradient boosting\n",
      "Best params: {'criterion': 'friedman_mse', 'loss': 'log_loss', 'max_depth': 22, 'n_estimators': 100}\n",
      "Best training accuracy: 0.814322193197556\n",
      "Validation set accuracy score for best params: 0.8200078771169752\n",
      "-------------------------------------------------------\n",
      "Classifier with best validation set accuracy: Random Forest\n"
     ]
    }
   ],
   "source": [
    "select_mod.choose(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.823195713835487"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_for = RandomForestClassifier(class_weight='balanced', random_state=21, criterion='gini', max_depth=47, n_estimators=195, n_jobs=12).fit(X_train, y_train)\n",
    "y_pred = ran_for.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение получилось выше чем в прошлый раз"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Однако что будет плохо предсказать, что блюдо вкусное, когда оно на самом деле невкусное, или же что блюдо невкусное, когда оно на самом деле вкусное. Второй вариант явно, для этого поменяем accuracy на precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8501211491865698"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro', labels=['great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вставим precision в наш автомтатический вычислятор"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection:\n",
    "    def __init__(self, grids, grid_dict):\n",
    "        self.result = None\n",
    "        self.grids = grids\n",
    "        self.grid_dict = grid_dict\n",
    "\n",
    "    def choose(self, X_train, y_train, X_valid, y_valid):\n",
    "        out = pd.DataFrame({'model_num': self.grid_dict.keys(),\n",
    "                                'model': self.grid_dict.values()})\n",
    "\n",
    "        def select_params(row):\n",
    "            ind = row['model_num']\n",
    "            print(f'Estimator: {self.grid_dict[ind]}')\n",
    "\n",
    "            self.grids[ind].fit(X_train, y_train)\n",
    "            print(f'Best params: {self.grids[ind].best_params_}')\n",
    "            y_predict = self.grids[ind].predict(X_valid)\n",
    "\n",
    "\n",
    "            score = precision_score(y_valid, y_predict, average='micro', labels=['great'])\n",
    "            print(f'Best training precision: {self.grids[ind].best_score_}')\n",
    "            print(f'Validation set precision score for best params: {score}')\n",
    "            print('-'*55)\n",
    "\n",
    "            return self.grids[ind].best_params_, score\n",
    "\n",
    "\n",
    "        out[['params', 'valid_score']] = out.progress_apply(select_params,\n",
    "                                                                axis=1,\n",
    "                                                                result_type='expand')\n",
    "        self.result = out\n",
    "        best_clf = out.model[out.valid_score.idxmax()]\n",
    "\n",
    "        print(f'Classifier with best validation set accuracy: {best_clf}')\n",
    "\n",
    "    def best_results(self):\n",
    "        return self.result.drop(columns='model_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43771bca8a8e4d0dbf8a2d4067460b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Random Forest\n",
      "Best params: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 49, 'n_estimators': 195}\n",
      "Best training precision: 0.8238767575486365\n",
      "Validation set precision score for best params: 0.827150428047289\n",
      "-------------------------------------------------------\n",
      "Estimator: Grid tree\n",
      "Best params: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 3}\n",
      "Best training precision: 0.8124507085980117\n",
      "Validation set precision score for best params: 0.8171956609079952\n",
      "-------------------------------------------------------\n",
      "Estimator: Gradient boosting\n",
      "Best params: {'criterion': 'friedman_mse', 'loss': 'log_loss', 'max_depth': 22, 'n_estimators': 100}\n",
      "Best training precision: 0.814322193197556\n",
      "Validation set precision score for best params: 0.8369839932603201\n",
      "-------------------------------------------------------\n",
      "Classifier with best validation set accuracy: Gradient boosting\n"
     ]
    }
   ],
   "source": [
    "select_mod = ModelSelection(grids, grid_dict)\n",
    "select_mod.choose(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8376733175515726"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_for = GradientBoostingClassifier(criterion='friedman_mse', loss='log_loss', max_depth=22, n_estimators=100).fit(X_train, y_train)\n",
    "y_pred = ran_for.predict(X_test)\n",
    "precision_score(y_test, y_pred, average='micro', labels=['great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_params = {'weights': [[1, 2], [1, 5], [3, 1], [5, 2]]}\n",
    "\n",
    "grid_vote = GridSearchCV(estimator=VotingClassifier(estimators=[('ran_for', RandomForestClassifier(class_weight=None, criterion='entropy', max_depth=49, n_estimators=195, random_state=21)), ('boosting', GradientBoostingClassifier(criterion='friedman_mse', loss='log_loss', max_depth=22, n_estimators=100))]), param_grid=vote_params, cv=5, n_jobs=12)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "bag_params = {'n_estimators': range(10, 30)}\n",
    "\n",
    "grid_bag = GridSearchCV(estimator=BaggingClassifier(base_estimator=GradientBoostingClassifier(criterion='friedman_mse', loss='log_loss', max_depth=22, n_estimators=100)), param_grid=bag_params, cv=5, n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [grid_bag, grid_vote]\n",
    "\n",
    "grid_dict = {0: 'Bagging', 1: 'Vote'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mod = ModelSelection(grids, grid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af44786d4e4a4ba9bb65d79ccc5a2e80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Bagging\n"
     ]
    }
   ],
   "source": [
    "select_mod.choose(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8480749219562955"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote = VotingClassifier(estimators=[('ran_for', RandomForestClassifier(class_weight='balanced', criterion='entropy', max_depth=45, n_estimators=160, random_state=21)), ('boosting', GradientBoostingClassifier(criterion='friedman_mse', loss='log_loss', max_depth=20, n_estimators=50, random_state=21))], weights=[3, 1]).fit(X_train, y_train)\n",
    "y_pred = vote.predict(X_test)\n",
    "precision_score(y_test, y_pred, average='micro', labels=['great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать эту модель, сделаем тренировку на всём объёме данных и сохраним её"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = VotingClassifier(estimators=[('ran_for', RandomForestClassifier(class_weight='balanced', criterion='entropy', max_depth=45, n_estimators=160, random_state=21)), ('boosting', GradientBoostingClassifier(criterion='friedman_mse', loss='log_loss', max_depth=20, n_estimators=50, random_state=21))], weights=[3, 1]).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "with open('vote_model.sav', 'wb') as f:\n",
    "    joblib.dump(vote, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
